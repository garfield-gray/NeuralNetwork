{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFiGQm0CsKIi"
   },
   "source": [
    "# Example 25.3\n",
    "\n",
    "Using Analytic Gradient for Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TBNt6HXt7tz"
   },
   "source": [
    "$$\n",
    "Z_{n,m} = f_1( X_{n,d}.W_{1\\;d,m}) \\\\\n",
    "O_{n,p} = f_2(Z_{n,m}.W_{2\\;m,p}) \\\\ \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{E} = \\frac{1}{n}\\sum^{n}_{i=1}\\frac{1}{2} \\left( \\mathbf Y_{i} - \\mathbf O_{i}\\right)^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{E} = \\frac{1}{n}\\sum^{n}_{i=1}\\sum^{p}_{j=1}\\left( -Y_{i,j}\\log O_{i,j}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6jXI1yQSJ8T"
   },
   "source": [
    "$$\n",
    "\\left(\\frac{\\partial\\mathcal{E}}{\\partial net_2}\\right)_{n,p}=\\left(\\frac{\\partial\\mathcal E}{\\partial f_2}\\right)_{n,p}\\odot\\left(\\frac{\\partial f_2}{\\partial net_2}\\right)_{n,p}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left(\\frac{\\partial \\mathcal{E}}{\\partial net_1}\\right)_{n,m}=\\left(\\left(\\frac{\\partial\\mathcal{E}}{\\partial net_2}\\right)_{n,p}\\cdot\n",
    "\\left(W_2^T\\right)_{p,m}\\right)\\odot\\left(\\frac{\\partial f_1}{\\partial net_1}\\right)_{n,m}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4L0u-9gDWuYk"
   },
   "source": [
    "$$\n",
    "\\left(\\nabla_{W_2}\\mathcal{E}\\right)_{m,p} = \\left(Z^T\\right)_{m,n} \\cdot \\left(\\frac{\\partial\\mathcal{E}}{\\partial net_2}\\right)_{n,p}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left(\\nabla_{W_1}\\mathcal{E}\\right)_{d,m} = \\left(X^T\\right)_{d,n} \\cdot \\left(\\frac{\\partial\\mathcal{E}}{\\partial net_1}\\right)_{n,m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYyJny9QWuWn"
   },
   "source": [
    "$$\n",
    "\\left(\\frac{\\partial\\mathcal{E}}{\\partial net_2}\\right)_{n,p}=\\frac{1}{n}( O - Y )_{n,p}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nEgndt4tlroJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def f1(x): return np.tanh(x)\n",
    "def df1(x): return 1 - np.power(np.tanh(x),2)\n",
    "def f2(x): return x\n",
    "def df2(x): return 1\n",
    "\n",
    "def aug(X):                                 #puts a 1 column in front of matrix\n",
    "    n = len(X)\n",
    "    X1 = np.ones((n,1))\n",
    "    return np.hstack((X1,X))\n",
    "\n",
    "def predict(X,W1,W2):\n",
    "    Z = f1( aug(X) @ W1)\n",
    "    O = f2( aug(Z) @ W2)\n",
    "    return O\n",
    "\n",
    "def update_pars(Y,X,W1,W2):\n",
    "    Xt = aug(X)\n",
    "    Z = f1( aug(X) @ W1)\n",
    "    Zt = aug(Z)\n",
    "    dZ = df1(Xt @ W1)\n",
    "    O = f2(Zt @ W2)\n",
    "    G2 = (O - Y)/len(Y)\n",
    "    dW2 = Zt.T @ G2\n",
    "    W2 -= eta * dW2\n",
    "    G1 = (G2 @ W2[1:].T)*dZ\n",
    "    dW1 = Xt.T @ G1\n",
    "    W1 -= eta * dW1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWk5qx9nkCsU"
   },
   "outputs": [],
   "source": [
    "n,d,m,p = 25,1,10,1\n",
    "\n",
    "X = np.random.uniform(-10,10,n).reshape((n,d))\n",
    "Y = np.sin(X) # n,d\n",
    "\n",
    "np.random.seed(11)\n",
    "W1 = np.random.randn(d+1,m)\n",
    "W2 = np.random.randn(m+1,p)\n",
    "\n",
    "eta = 1e-1  # step size (learning rate)\n",
    "num_steps = int(1e5)\n",
    "\n",
    "for i in tqdm(range(num_steps)):\n",
    "    # idx = np.random.choice(np.arange(n),5)\n",
    "    # update_pars(Y[idx],X[idx],W1,W2)\n",
    "    update_pars(Y,X,W1,W2)\n",
    "\n",
    "t = np.arange(-10,10,0.1)\n",
    "yt = np.sin(t)\n",
    "yp = predict(t[:,None],W1,W2)\n",
    "\n",
    "plt.plot(t,yp,'b')\n",
    "plt.plot(t,yt,':r')\n",
    "plt.scatter(X,Y,ec='red',fc='none')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aADJ7lGn4rDA"
   },
   "source": [
    "-------------------------------------\n",
    "# Example 25.3\n",
    "\n",
    "Using Numerical Gradient for Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWD-z_A84zV-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def f1(x): return np.tanh(x)\n",
    "def df1(x): return 1 - np.power(np.tanh(x),2)\n",
    "def f2(x): return x\n",
    "def df2(x): return 1\n",
    "\n",
    "def aug(X):\n",
    "    n = len(X)\n",
    "    X1 = np.ones((n,1))\n",
    "    return np.hstack((X1,X))\n",
    "\n",
    "def predict(X,W1,W2):\n",
    "    Z = f1( aug(X) @ W1)\n",
    "    O = f2( aug(Z) @ W2)\n",
    "    return O\n",
    "\n",
    "def loss(Y,X,W1,W2):\n",
    "    return np.average(np.power(predict(X,W1,W2)-Y,2))\n",
    "\n",
    "def update(x,i,j):\n",
    "    h=1e-5\n",
    "    x0 = x[i,j].copy()\n",
    "    x[i,j] = x0 + h\n",
    "    f2 = loss(Y,X,W1,W2)\n",
    "    x[i,j] = x0 - h\n",
    "    f1 = loss(Y,X,W1,W2)\n",
    "    g = (f2-f1)/(2*h)\n",
    "    x0 = x0 - eta*g\n",
    "    x[i,j] = x0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0MVzD3C44JH"
   },
   "outputs": [],
   "source": [
    "n,d,m,p = 25,1,10,1\n",
    "\n",
    "X = np.random.uniform(-np.pi,np.pi,n).reshape((n,d))\n",
    "Y = np.sin(X) # n,d\n",
    "\n",
    "np.random.seed(111)\n",
    "W1 = np.random.randn(d+1,m)\n",
    "W2 = np.random.randn(m+1,p)\n",
    "\n",
    "num_steps = int(1e4)\n",
    "eta = 1e-2\n",
    "\n",
    "print(f\"loss = {loss(Y,X,W1,W2).round(3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zClA92bs4-wg"
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(num_steps)):\n",
    "\n",
    "    a,b = W1.shape\n",
    "    for i in range(a):\n",
    "        for j in range(b):\n",
    "            update(W1,i,j)\n",
    "    a,b = W2.shape\n",
    "    for i in range(a):\n",
    "        for j in range(b):\n",
    "            update(W2,i,j)\n",
    "\n",
    "\n",
    "t = np.arange(-2*np.pi,2*np.pi,0.1)\n",
    "yt = np.sin(t)\n",
    "yp = predict(t[:,None],W1,W2)\n",
    "\n",
    "print(f\"training loss = {loss(Y,X,W1,W2).round(3)}\")\n",
    "print(f\"testing loss = {loss(yt[:,None],t[:,None],W1,W2).round(3)}\")\n",
    "\n",
    "plt.plot(t,yp,'b')\n",
    "plt.plot(t,yt,':r')\n",
    "plt.scatter(X,Y,ec='red',fc='none')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCsVMQZ_Xcdi"
   },
   "source": [
    "----------------------\n",
    "# Nonlinear Regression\n",
    "\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "y_i & = & f(x_i) + e_i \\\\\n",
    "f(x_i)_{j+1} & = & f(x_i)_{j} + \\frac{\\partial f(x_i)_j}{\\partial a_0}\\Delta a_0 + \\frac{\\partial f(x_i)_j}{\\partial a_1}\\Delta a_1 + e_i \\\\\n",
    "y_i - f(x_i)_{j} & = & \\frac{\\partial f(x_i)_j}{\\partial a_0}\\Delta a_0 + \\frac{\\partial f(x_i)_j}{\\partial a_1}\\Delta a_1 + e_i \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left[y - f(x)\\right]_{i,j} = \\left[\\frac{\\partial f(x)}{\\partial a_0} \\; \\frac{\\partial f(x)}{\\partial a_1}\\right]_{i,j}\\left[\\begin{matrix}\\Delta a_0 \\\\ \\Delta a_1\\end{matrix}\\right]+[e]_i \\rightarrow \\{D\\} = [Z]\\{\\Delta A\\}+\\{E\\}\\\\\n",
    "\\{\\Delta A\\} = \\left[Z^T\\,Z\\right]^{-1}\\{[Z]^T\\{D\\}\\} \\\\\n",
    "\\{A_{j+1}\\} = \\{A_{j}\\} + \\{\\Delta A_j\\} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvZ0Qgv4m6dc"
   },
   "outputs": [],
   "source": [
    "from numpy.core.fromnumeric import size\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return a0*(1-np.exp(-a1*x))\n",
    "\n",
    "def df0(x):\n",
    "    return (1-np.exp(-a1*x))\n",
    "\n",
    "def df1(x):\n",
    "    return a0*x*np.exp(-a1*x)\n",
    "\n",
    "x = np.array([0.25,0.75,1.25,1.75,2.25])[:,None]\n",
    "y = np.array([0.28,0.57,0.68,0.74,0.79])[:,None]\n",
    "\n",
    "\n",
    "X = np.arange(0,2.5,0.1)\n",
    "\n",
    "eps = 0.0001\n",
    "a0,a1 =1.,1.\n",
    "for i in range(1000):\n",
    "    D = y-f(x)\n",
    "    Z = np.hstack([df0(x),df1(x)])\n",
    "    dA = np.linalg.inv( Z.T @ Z ) @ (Z.T @ D)\n",
    "    a0_ = a0 + dA[0]\n",
    "    a1_ = a1 + dA[1]\n",
    "    if (np.abs(a0-a0_) < eps) & (np.abs(a0-a0_) < eps ): \n",
    "        break\n",
    "    a0,a1 = a0_,a1_\n",
    "    Y = f(X)\n",
    "\n",
    "    plt.gca().cla() \n",
    "    plt.scatter(x,y,fc='none',ec='red',lw=2,alpha=0.5)\n",
    "    plt.xlabel(\"x\",size=20)\n",
    "    plt.ylabel(\"y\",size=20)\n",
    "    plt.plot(X,Y,'--b',lw=2,alpha=0.5)\n",
    "    plt.text(0.5,0.1,f'y = {a0[0].round(2)} $(1-e^{{-{a1[0].round(2)}\\;x}})$',size=20)\n",
    "    time.sleep(2)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf()) \n",
    "\n",
    "\n",
    "display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_uE9XO3NqZl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "LFiGQm0CsKIi",
    "hCsVMQZ_Xcdi"
   ],
   "name": "14001006_C25_NeuralNetworks_P25.3.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
